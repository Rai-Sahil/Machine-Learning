{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Applying Gradient Descent from Scratch for Linear and Logistic Reg"
      ],
      "metadata": {
        "id": "fqDTiKaFyQvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "qh96mJNVGcmL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_X = np.array([22, 28, 32, 40, 45, 50, 55, 60, 65, 70, 18, 38, 42, 48, 58])\n",
        "input_y = np.array([0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1])\n",
        "\n",
        "# input_X = (input_X - np.mean(input_X)) / np.std(input_X)\n",
        "# input_y = (input_y - np.mean(input_y)) / np.std(input_y)"
      ],
      "metadata": {
        "id": "Wu-WjTjlGgj6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Without using sum function\n",
        "\n",
        "\n",
        "*   First use derivative of loss function (MSE)\n",
        "*   Then use learning_rate to adjest the new m, b or theta\n",
        "    * m = theta[0] & b = theta[1]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "y2aFQnltyfGH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X, y, learning_rate, epochs):\n",
        "  theta = [0, 0]\n",
        "\n",
        "  for _ in range(epochs):\n",
        "    temp_m, temp_b = 0, 0\n",
        "    for i in range(len(X)):\n",
        "      temp_m -= X[i] * (y[i] - (theta[0] * X[i] + theta[1]))\n",
        "      temp_b -= y[i] - (theta[0] * X[i] + theta[1])\n",
        "\n",
        "    temp_b = temp_b * (2 / len(X))\n",
        "    temp_m = temp_m * (2 / len(X))\n",
        "\n",
        "    theta[0] = theta[0] - (learning_rate * temp_m)\n",
        "    theta[1] = theta[1] - (learning_rate * temp_b)\n",
        "\n",
        "  return theta[0], theta[1]"
      ],
      "metadata": {
        "id": "FrJKiJP1GwqR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent2(X, y, learning_rate, epochs):\n",
        "  theta = [0, 0]\n",
        "\n",
        "  for j in range(epochs):\n",
        "    temp_m = -(2/len(X)) * sum(X * (y - (theta[0] * X + theta[1])))\n",
        "    temp_b = -(2/len(X)) * sum(y - (theta[0] * X + theta[1]))\n",
        "\n",
        "    theta[0] = theta[0] - (learning_rate * temp_m)\n",
        "    theta[1] = theta[1] - (learning_rate * temp_b)\n",
        "\n",
        "  return theta[0], theta[1]"
      ],
      "metadata": {
        "id": "_W4NZkOMQgXC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m, b = gradient_descent(input_X, input_y, 0.0_001, 1_00_000)\n",
        "print(f\"m = {m}\\n b = {b}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slt3_TVuG71E",
        "outputId": "b4bf5f4f-82d5-469c-a81c-39d744a8344e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "m = 0.015630305840212175\n",
            " b = -0.09767045464771683\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Making prediction from my gradient descent model for Logistic model"
      ],
      "metadata": {
        "id": "yyNeKoa0zD8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = []\n",
        "for x in input_X:\n",
        "  y = (m * x) + b\n",
        "\n",
        "  if y >= 0.5:\n",
        "    predictions.append(1)\n",
        "  else:\n",
        "    predictions.append(0)\n",
        "\n",
        "# Get the accuracy\n",
        "accuracy = np.sum(predictions == input_y) / len(input_y)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1M2uKVqDKujY",
        "outputId": "8b2a3dfe-8a9b-4427-bf73-e847652bc611"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparing my accuracy with sklearn log reg model models"
      ],
      "metadata": {
        "id": "F2e_IQKmzRhu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegression()\n",
        "model.fit(input_X.reshape(-1, 1), input_y)\n",
        "model.score(input_X.reshape(-1, 1), input_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfhMV5YwqHjC",
        "outputId": "b0eeff9b-3950-4452-9bae-d79d5d6543ba"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}